import json
import logging
import os
import re
import signal
import sqlite3
import sys
import time
from dataclasses import dataclass
from datetime import datetime, timezone
from tempfile import NamedTemporaryFile
from typing import Dict, List, Optional, Tuple, Any
from urllib.error import HTTPError, URLError
from urllib.parse import quote_plus
from urllib.request import Request, urlopen
from xml.etree import ElementTree

import matplotlib
matplotlib.use('Agg')  # Non-GUI backend for Render.com
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pandas_datareader.data as web
import requests
import yfinance as yf
from dotenv import load_dotenv
from telegram import KeyboardButton, ReplyKeyboardMarkup, ReplyKeyboardRemove, Update
from telegram.ext import (
    Application,
    CommandHandler,
    ContextTypes,
    ConversationHandler,
    MessageHandler,
    filters,
)

logging.basicConfig(
    format="%(asctime)s | %(name)s | %(levelname)s | %(message)s",
    level=logging.INFO,
    handlers=[
        logging.StreamHandler(sys.stdout),  # For Render.com logs
    ]
)
logger = logging.getLogger(__name__)


# ============ CACHE SYSTEM ============
class SimpleCache:
    """Simple in-memory cache with TTL (time-to-live) support."""
    
    def __init__(self):
        self.cache: Dict[str, Tuple[Any, float]] = {}
    
    def get(self, key: str, ttl_seconds: int = 600) -> Optional[Any]:
        """Get cached value if it exists and is not expired."""
        if key not in self.cache:
            return None
        
        value, timestamp = self.cache[key]
        if time.time() - timestamp > ttl_seconds:
            del self.cache[key]
            return None
        
        return value
    
    def set(self, key: str, value: Any) -> None:
        """Store value in cache with current timestamp."""
        self.cache[key] = (value, time.time())
    
    def clear(self) -> None:
        """Clear all cache."""
        self.cache.clear()
    
    def cleanup(self, ttl_seconds: int = 600) -> int:
        """Remove expired items, return count of removed items."""
        now = time.time()
        expired_keys = [
            key for key, (_, timestamp) in self.cache.items()
            if now - timestamp > ttl_seconds
        ]
        for key in expired_keys:
            del self.cache[key]
        return len(expired_keys)


# Global cache instances
market_data_cache = SimpleCache()
news_cache = SimpleCache()

# Cache TTL settings (in seconds)
MARKET_DATA_CACHE_TTL = int(os.getenv("MARKET_DATA_CACHE_TTL", "600"))  # 10 minutes
NEWS_CACHE_TTL = int(os.getenv("NEWS_CACHE_TTL", "1800"))  # 30 minutes

MENU_STOCK = "üìà –ê–Ω–∞–ª–∏–∑ –∞–∫—Ü–∏–∏"
MENU_PORTFOLIO = "üíº –ê–Ω–∞–ª–∏–∑ –ø–æ—Ä—Ç—Ñ–µ–ª—è"
MENU_MY_PORTFOLIO = "üìÇ –ú–æ–π –ø–æ—Ä—Ç—Ñ–µ–ª—å"
MENU_COMPARE = "üîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–∫—Ü–∏–π"
MENU_HELP = "‚ÑπÔ∏è –ü–æ–º–æ—â—å"
MENU_CANCEL = "‚ùå –û—Ç–º–µ–Ω–∞"

CHOOSING, WAITING_STOCK, WAITING_PORTFOLIO, WAITING_COMPARISON = range(4)

DB_PATH = os.getenv("PORTFOLIO_DB_PATH", "portfolio.db")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini").strip() or "gpt-4o-mini"


@dataclass
class Position:
    ticker: str
    quantity: float
    avg_price: Optional[float]


def main_keyboard() -> ReplyKeyboardMarkup:
    return ReplyKeyboardMarkup(
        [
            [KeyboardButton(MENU_STOCK), KeyboardButton(MENU_PORTFOLIO)],
            [KeyboardButton(MENU_MY_PORTFOLIO), KeyboardButton(MENU_COMPARE)],
            [KeyboardButton(MENU_HELP), KeyboardButton(MENU_CANCEL)],
        ],
        resize_keyboard=True,
    )


def safe_float(value: str) -> Optional[float]:
    try:
        return float(value.replace(",", "."))
    except ValueError:
        return None


def init_db() -> None:
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS user_portfolios (
                user_id INTEGER PRIMARY KEY,
                raw_text TEXT NOT NULL,
                updated_at TEXT NOT NULL
            )
            """
        )
        conn.commit()


def save_portfolio(user_id: int, raw_text: str) -> None:
    now = datetime.now(timezone.utc).isoformat()
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute(
            """
            INSERT INTO user_portfolios(user_id, raw_text, updated_at)
            VALUES (?, ?, ?)
            ON CONFLICT(user_id) DO UPDATE SET
                raw_text=excluded.raw_text,
                updated_at=excluded.updated_at
            """,
            (user_id, raw_text, now),
        )
        conn.commit()


def get_saved_portfolio(user_id: int) -> Optional[str]:
    with sqlite3.connect(DB_PATH) as conn:
        row = conn.execute(
            "SELECT raw_text FROM user_portfolios WHERE user_id = ?", (user_id,)
        ).fetchone()
    return row[0] if row else None


def parse_portfolio_text(text: str) -> List[Position]:
    positions: List[Position] = []
    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line:
            continue

        normalized = re.sub(r"[,:;]+", " ", line)
        parts = [p for p in normalized.split() if p]

        if len(parts) < 2:
            continue

        ticker = parts[0].upper()
        quantity = safe_float(parts[1])
        avg_price = safe_float(parts[2]) if len(parts) >= 3 else None

        if quantity is None or quantity <= 0:
            continue

        positions.append(Position(ticker=ticker, quantity=quantity, avg_price=avg_price))

    return positions


def load_data_from_stooq(ticker: str, period: str) -> Optional[pd.DataFrame]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Stooq —á–µ—Ä–µ–∑ pandas_datareader."""
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∞—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–∏–æ–¥–∞
        from datetime import datetime, timedelta
        end_date = datetime.now()
        period_days = {
            "1d": 1, "5d": 5, "1mo": 30, "3mo": 90,
            "6mo": 180, "1y": 365, "2y": 730, "5y": 1825, "max": 3650
        }
        days = period_days.get(period, 180)
        start_date = end_date - timedelta(days=days)
        
        logger.info("Trying to load %s from Stooq (fallback)", ticker)
        df = web.DataReader(ticker, 'stooq', start_date, end_date)
        
        if df.empty:
            return None
            
        # Stooq –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å –¥—Ä—É–≥–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∫–æ–ª–æ–Ω–æ–∫, –Ω—É–∂–Ω–æ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—É
        df.columns = [col.capitalize() for col in df.columns]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (Stooq –º–æ–∂–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ)
        df = df.sort_index()
        
        if 'Close' in df.columns and len(df) >= 1:
            logger.info("Successfully loaded %d rows from Stooq for %s", len(df), ticker)
            return df.dropna()
        return None
    except Exception as exc:
        logger.warning("Stooq fallback failed for %s: %s", ticker, exc)
        return None


def load_data_from_sec_edgar(ticker: str) -> Optional[pd.DataFrame]:
    """–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ SEC EDGAR (—Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è US –∫–æ–º–ø–∞–Ω–∏–π)."""
    try:
        # SEC EDGAR Company Tickers API
        logger.info("Trying to get company info from SEC EDGAR for %s", ticker)
        
        # –ü–æ–ª—É—á–∞–µ–º CIK (Central Index Key) –∫–æ–º–ø–∞–Ω–∏–∏
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; FinBot/1.0; +http://example.com/bot)',
            'Accept': 'application/json'
        }
        
        # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–º–ø–∞–Ω–∏–π
        tickers_url = "https://www.sec.gov/files/company_tickers.json"
        response = requests.get(tickers_url, headers=headers, timeout=10)
        
        if response.status_code != 200:
            return None
            
        companies = response.json()
        cik = None
        
        # –ò—â–µ–º CIK –¥–ª—è –Ω–∞—à–µ–≥–æ —Ç–∏–∫–µ—Ä–∞
        for company in companies.values():
            if company.get('ticker', '').upper() == ticker.upper():
                cik = str(company['cik_str']).zfill(10)
                break
        
        if not cik:
            logger.info("Ticker %s not found in SEC database (may be non-US)", ticker)
            return None
        
        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
        facts_url = f"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json"
        response = requests.get(facts_url, headers=headers, timeout=15)
        
        if response.status_code != 200:
            return None
            
        logger.info("SEC EDGAR data available for %s (CIK: %s)", ticker, cik)
        # SEC EDGAR –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ü–µ–Ω—ã, —Ç–æ–ª—å–∫–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        # –ü–æ—ç—Ç–æ–º—É –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None –∏ –ø–æ–ª–∞–≥–∞–µ–º—Å—è –Ω–∞ Stooq
        return None
        
    except Exception as exc:
        logger.warning("SEC EDGAR lookup failed for %s: %s", ticker, exc)
        return None


def compute_rsi(close: pd.Series, period: int = 14) -> pd.Series:
    if isinstance(close, pd.DataFrame):
        close = close.iloc[:, 0]
    close = pd.Series(close).astype(float)
    delta = close.diff()
    gain = np.where(delta > 0, delta, 0.0)
    loss = np.where(delta < 0, -delta, 0.0)
    avg_gain = pd.Series(gain, index=close.index).rolling(period).mean()
    avg_loss = pd.Series(loss, index=close.index).rolling(period).mean()
    rs = avg_gain / avg_loss.replace(0, np.nan)
    rsi = 100 - (100 / (1 + rs))
    return rsi.fillna(50)


def load_market_data(
    ticker: str, period: str, interval: str, min_rows: int = 1
) -> tuple[Optional[pd.DataFrame], Optional[str]]:
    last_exc: Optional[Exception] = None
    rate_limited = False
    
    # Check cache first
    cache_key = f"{ticker}_{period}_{interval}"
    cached_data = market_data_cache.get(cache_key, MARKET_DATA_CACHE_TTL)
    if cached_data is not None:
        logger.info("Cache hit for %s (period=%s, interval=%s)", ticker, period, interval)
        return cached_data, None
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º yfinance
    for attempt in range(3):
        try:
            data = yf.download(
                ticker,
                period=period,
                interval=interval,
                progress=False,
                auto_adjust=True,
                threads=False,
            )
            if not data.empty and "Close" in data.columns and len(data.dropna()) >= min_rows:
                data = data.dropna().copy()
                market_data_cache.set(cache_key, data)
                return data, None
        except Exception as exc:
            last_exc = exc
            if "rate limit" in str(exc).lower() or "too many requests" in str(exc).lower():
                rate_limited = True
                break

        try:
            data = yf.Ticker(ticker).history(period=period, interval=interval, auto_adjust=True)
            if not data.empty and "Close" in data.columns and len(data.dropna()) >= min_rows:
                data = data.dropna().copy()
                market_data_cache.set(cache_key, data)
                return data, None
        except Exception as exc:
            last_exc = exc
            if "rate limit" in str(exc).lower() or "too many requests" in str(exc).lower():
                rate_limited = True
                break

        time.sleep(1.2 * (attempt + 1))

    # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ rate limit, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    if rate_limited:
        logger.warning("Rate limit detected for %s, trying fallback sources", ticker)
        
        # –ü—Ä–æ–±—É–µ–º SEC EDGAR (–≤ –æ—Å–Ω–æ–≤–Ω–æ–º –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, —á—Ç–æ —ç—Ç–æ US –∫–æ–º–ø–∞–Ω–∏—è)
        sec_data = load_data_from_sec_edgar(ticker)
        
        # –ü—Ä–æ–±—É–µ–º Stooq
        stooq_data = load_data_from_stooq(ticker, period)
        if stooq_data is not None and "Close" in stooq_data.columns and len(stooq_data) >= min_rows:
            logger.info("Successfully loaded data from Stooq for %s", ticker)
            market_data_cache.set(cache_key, stooq_data)
            return stooq_data, None
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–º–æ–≥–ª–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º rate_limit
        logger.warning("All fallback sources failed for %s", ticker)
        return None, "rate_limit"

    if last_exc is not None:
        logger.warning("Cannot load market data for %s: %s", ticker, last_exc)
    return None, "not_found_or_no_data"


def stock_snapshot(ticker: str) -> tuple[Optional[pd.DataFrame], Optional[str]]:
    data, reason = load_market_data(ticker, period="6mo", interval="1d", min_rows=30)
    if data is None or "Close" not in data.columns:
        return None, reason or "not_found_or_no_data"
    if isinstance(data["Close"], pd.DataFrame):
        data["Close"] = data["Close"].iloc[:, 0]
    data["SMA20"] = data["Close"].rolling(20).mean()
    data["SMA50"] = data["Close"].rolling(50).mean()
    data["RSI14"] = compute_rsi(data["Close"], 14)
    return data, None


def stock_analysis_text(ticker: str, df: pd.DataFrame) -> str:
    last = df.iloc[-1]
    prev = df.iloc[-2]

    close = float(last["Close"])
    daily_change = (close / float(prev["Close"]) - 1) * 100
    sma20 = float(last["SMA20"])
    sma50 = float(last["SMA50"])
    rsi = float(last["RSI14"])

    trend = "–≤–æ—Å—Ö–æ–¥—è—â–∏–π" if sma20 > sma50 else "–Ω–∏—Å—Ö–æ–¥—è—â–∏–π"

    signals = []
    if rsi > 70:
        signals.append("RSI –≤—ã—à–µ 70: –∞–∫—Ç–∏–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω.")
    elif rsi < 30:
        signals.append("RSI –Ω–∏–∂–µ 30: –∞–∫—Ç–∏–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω.")
    else:
        signals.append("RSI –≤ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–π –∑–æ–Ω–µ.")

    if close > sma20 > sma50:
        signals.append("–¶–µ–Ω–∞ –≤—ã—à–µ SMA20 –∏ SMA50: —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ —Å–∏–ª—å–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞.")
    elif close < sma20 < sma50:
        signals.append("–¶–µ–Ω–∞ –Ω–∏–∂–µ SMA20 –∏ SMA50: —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ —Å–ª–∞–±–∞—è –¥–∏–Ω–∞–º–∏–∫–∞.")
    else:
        signals.append("–°–∏–≥–Ω–∞–ª—ã —Å–º–µ—à–∞–Ω–Ω—ã–µ: –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞ —Å–ª–∞–±–æ–µ.")

    risk_line = (
        "–ò–¥–µ—è: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏–º–∏—Ç—ã —Ä–∏—Å–∫–∞ –∏ –Ω–µ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–æ –æ–¥–Ω–æ–º—É –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—É."
    )

    return (
        f"{ticker}\n"
        f"–¶–µ–Ω–∞: {close:.2f}\n"
        f"–ò–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞ –¥–µ–Ω—å: {daily_change:+.2f}%\n"
        f"–¢—Ä–µ–Ω–¥ –ø–æ SMA(20/50): {trend}\n"
        f"RSI(14): {rsi:.1f}\n\n"
        "–ö–ª—é—á–µ–≤—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:\n"
        f"- {signals[0]}\n"
        f"- {signals[1]}\n"
        f"- {risk_line}\n"
    )


def render_stock_chart(ticker: str, df: pd.DataFrame) -> str:
    fig, (ax1, ax2) = plt.subplots(
        2, 1, figsize=(10, 7), sharex=True, gridspec_kw={"height_ratios": [3, 1]}
    )

    ax1.plot(df.index, df["Close"], label="Close", linewidth=1.8)
    ax1.plot(df.index, df["SMA20"], label="SMA20", linestyle="--", linewidth=1.2)
    ax1.plot(df.index, df["SMA50"], label="SMA50", linestyle="--", linewidth=1.2)
    ax1.set_title(f"{ticker}: —Ü–µ–Ω–∞ –∏ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ (6 –º–µ—Å—è—Ü–µ–≤)")
    ax1.grid(alpha=0.25)
    ax1.legend()

    ax2.plot(df.index, df["RSI14"], label="RSI14", color="purple", linewidth=1.2)
    ax2.axhline(70, color="red", linestyle="--", linewidth=0.8)
    ax2.axhline(30, color="green", linestyle="--", linewidth=0.8)
    ax2.set_ylim(0, 100)
    ax2.grid(alpha=0.25)
    ax2.legend()

    fig.tight_layout()
    with NamedTemporaryFile(delete=False, suffix=".png") as tmp:
        fig.savefig(tmp.name, dpi=140)
        chart_path = tmp.name
    plt.close(fig)
    return chart_path


def compare_stocks(tickers: List[str], period: str = "6mo") -> tuple[Optional[str], Optional[str]]:
    """Compare multiple stocks: correlation, relative performance, chart."""
    if len(tickers) < 2:
        return None, "–ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 —Ç–∏–∫–µ—Ä–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"
    
    if len(tickers) > 5:
        return None, "–ú–∞–∫—Å–∏–º—É–º 5 —Ç–∏–∫–µ—Ä–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"
    
    # Load data for all tickers
    data_dict = {}
    for ticker in tickers:
        data, reason = load_market_data(ticker, period=period, interval="1d", min_rows=30)
        if data is None or "Close" not in data.columns:
            return None, f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {ticker}"
        data_dict[ticker] = data["Close"]
    
    # Combine into single DataFrame
    prices_df = pd.DataFrame(data_dict).dropna()
    
    if len(prices_df) < 30:
        return None, "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 30 –¥–Ω–µ–π)"
    
    # Calculate returns
    returns = prices_df.pct_change().dropna()
    
    # Correlation matrix
    corr_matrix = returns.corr()
    
    # Normalize prices to 100 at start (relative performance)
    normalized = (prices_df / prices_df.iloc[0]) * 100
    
    # Calculate statistics
    total_return = {}
    volatility = {}
    for ticker in tickers:
        total_return[ticker] = ((prices_df[ticker].iloc[-1] / prices_df[ticker].iloc[0]) - 1) * 100
        volatility[ticker] = returns[ticker].std() * np.sqrt(252) * 100  # Annualized
    
    # Create comparison chart
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), gridspec_kw={"height_ratios": [2, 1]})
    
    # Plot normalized prices
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
    for i, ticker in enumerate(tickers):
        ax1.plot(normalized.index, normalized[ticker], label=ticker, 
                linewidth=2, color=colors[i % len(colors)])
    
    ax1.set_title("–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ –∞–∫—Ü–∏–π (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∫ 100)", fontsize=14, fontweight='bold')
    ax1.set_ylabel("–ò–Ω–¥–µ–∫—Å (—Å—Ç–∞—Ä—Ç = 100)")
    ax1.grid(alpha=0.3)
    ax1.legend(loc='best')
    ax1.axhline(100, color='gray', linestyle='--', linewidth=0.8, alpha=0.5)
    
    # Plot correlation heatmap
    im = ax2.imshow(corr_matrix, cmap='RdYlGn', vmin=-1, vmax=1, aspect='auto')
    ax2.set_xticks(range(len(tickers)))
    ax2.set_yticks(range(len(tickers)))
    ax2.set_xticklabels(tickers)
    ax2.set_yticklabels(tickers)
    ax2.set_title("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π", fontsize=12)
    
    # Add correlation values
    for i in range(len(tickers)):
        for j in range(len(tickers)):
            text = ax2.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',
                           ha="center", va="center", color="black", fontsize=9)
    
    fig.colorbar(im, ax=ax2, label='–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è')
    fig.tight_layout()
    
    with NamedTemporaryFile(delete=False, suffix=".png") as tmp:
        fig.savefig(tmp.name, dpi=140)
        chart_path = tmp.name
    plt.close(fig)
    
    # Generate text summary
    lines = ["üìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∞–∫—Ü–∏–π\n"]
    lines.append(f"–ü–µ—Ä–∏–æ–¥: {period}, —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö: {len(prices_df)}\n")
    
    lines.append("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    sorted_by_return = sorted(total_return.items(), key=lambda x: x[1], reverse=True)
    for ticker, ret in sorted_by_return:
        vol = volatility[ticker]
        lines.append(f"- {ticker}: –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å {ret:+.2f}%, –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å {vol:.1f}%")
    
    lines.append("\n–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è (–Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –ø–∞—Ä—ã):")
    corr_pairs = []
    for i in range(len(tickers)):
        for j in range(i+1, len(tickers)):
            corr_pairs.append((tickers[i], tickers[j], corr_matrix.iloc[i, j]))
    
    corr_pairs = sorted(corr_pairs, key=lambda x: abs(x[2]), reverse=True)
    for t1, t2, corr in corr_pairs[:3]:
        lines.append(f"- {t1} ‚Üî {t2}: {corr:.2f}")
    
    lines.append("\n–í—ã–≤–æ–¥—ã:")
    if max(abs(c[2]) for c in corr_pairs) > 0.7:
        lines.append("- –í—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: –∞–∫—Ü–∏–∏ –¥–≤–∏–∂—É—Ç—Å—è –ø–æ—Ö–æ–∂–µ (–¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∏–∑–∫–∞—è)")
    elif max(abs(c[2]) for c in corr_pairs) < 0.3:
        lines.append("- –ù–∏–∑–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: —Ö–æ—Ä–æ—à–∞—è –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ—Ä—Ç—Ñ–µ–ª—è")
    
    best_ticker = sorted_by_return[0][0]
    worst_ticker = sorted_by_return[-1][0]
    lines.append(f"- –õ–∏–¥–µ—Ä: {best_ticker} (+{sorted_by_return[0][1]:.1f}%)")
    lines.append(f"- –ê—É—Ç—Å–∞–π–¥–µ—Ä: {worst_ticker} ({sorted_by_return[-1][1]:+.1f}%)")
    
    lines.append("\n–ù–µ —è–≤–ª—è–µ—Ç—Å—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–æ–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–µ–π.")
    
    return chart_path, "\n".join(lines)


def ticker_news(ticker: str, limit: int = 5) -> List[Dict[str, str]]:
    # Check cache first
    cache_key = f"news_{ticker}_{limit}"
    cached_news = news_cache.get(cache_key, NEWS_CACHE_TTL)
    if cached_news is not None:
        logger.info("Cache hit for news: %s", ticker)
        return cached_news
    
    try:
        raw_news = yf.Ticker(ticker).news or []
    except Exception as exc:
        logger.warning("Cannot load news for %s: %s", ticker, exc)
        raw_news = []

    items: List[Dict[str, str]] = []
    for item in raw_news:
        parsed = _parse_yf_news_item(item)
        if parsed:
            items.append(parsed)
        if len(items) >= limit:
            break

    if len(items) < limit:
        for item in yahoo_rss_news(ticker, limit=limit * 2):
            if not _is_duplicate_news(items, item):
                items.append(item)
            if len(items) >= limit:
                break

    result = items[:limit]
    news_cache.set(cache_key, result)
    return result


def _parse_yf_news_item(item: Dict) -> Optional[Dict[str, str]]:
    content = item.get("content") if isinstance(item.get("content"), dict) else {}
    canonical = content.get("canonicalUrl") if isinstance(content.get("canonicalUrl"), dict) else {}
    clickthrough = item.get("clickThroughUrl") if isinstance(item.get("clickThroughUrl"), dict) else {}
    provider = content.get("provider") if isinstance(content.get("provider"), dict) else {}

    title = (
        item.get("title")
        or content.get("title")
        or content.get("description")
        or "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
    )
    publisher = item.get("publisher") or provider.get("displayName") or "–ò—Å—Ç–æ—á–Ω–∏–∫"
    link = (
        item.get("link")
        or canonical.get("url")
        or clickthrough.get("url")
        or ""
    )

    date = ""
    ts = item.get("providerPublishTime")
    if isinstance(ts, (int, float)):
        date = datetime.utcfromtimestamp(ts).strftime("%Y-%m-%d")
    elif isinstance(content.get("pubDate"), str):
        raw = content["pubDate"].replace("Z", "+00:00")
        try:
            date = datetime.fromisoformat(raw).strftime("%Y-%m-%d")
        except ValueError:
            date = content["pubDate"][:10]

    if not title and not link:
        return None
    return {"title": title, "publisher": publisher, "date": date, "link": link}


def _is_duplicate_news(existing: List[Dict[str, str]], candidate: Dict[str, str]) -> bool:
    c_title = candidate.get("title", "").strip().lower()
    c_link = candidate.get("link", "").strip().lower()
    for item in existing:
        title_match = c_title and item.get("title", "").strip().lower() == c_title
        link_match = c_link and item.get("link", "").strip().lower() == c_link
        if title_match or link_match:
            return True
    return False


def yahoo_rss_news(ticker: str, limit: int = 5) -> List[Dict[str, str]]:
    url = (
        "https://feeds.finance.yahoo.com/rss/2.0/headline"
        f"?s={quote_plus(ticker)}&region=US&lang=en-US"
    )
    req = Request(
        url,
        headers={
            "User-Agent": "Mozilla/5.0",
            "Accept": "application/rss+xml, application/xml;q=0.9, */*;q=0.8",
        },
        method="GET",
    )
    try:
        with urlopen(req, timeout=12) as resp:
            payload = resp.read()
        root = ElementTree.fromstring(payload)
    except Exception as exc:
        logger.warning("RSS news fallback failed for %s: %s", ticker, exc)
        return []

    items: List[Dict[str, str]] = []
    for node in root.findall("./channel/item"):
        title = (node.findtext("title") or "").strip() or "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
        link = (node.findtext("link") or "").strip()
        pub = (node.findtext("pubDate") or "").strip()
        date = ""
        if pub:
            try:
                date = datetime.strptime(pub, "%a, %d %b %Y %H:%M:%S %z").strftime("%Y-%m-%d")
            except ValueError:
                date = pub[:16]
        item = {"title": title, "publisher": "Yahoo Finance", "date": date, "link": link}
        items.append(item)
        if len(items) >= limit:
            break

    return items


def fallback_news_summary(news: List[Dict[str, str]]) -> str:
    if not news:
        return "AI-–æ–±–∑–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π: –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ, –∞–Ω–∞–ª–∏–∑ –ø–æ –Ω–æ–≤–æ—Å—Ç—è–º –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω."

    lines = ["AI-–æ–±–∑–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π (–±–∞–∑–æ–≤—ã–π):"]
    for item in news[:3]:
        source = f"{item['publisher']} {item['date']}".strip()
        lines.append(f"- {item['title']} ({source})")

    lines.append("–í—ã–≤–æ–¥: –ø—Ä–æ–≤–µ—Ä—å, –∫–∞–∫ —ç—Ç–∏ —Å–æ–±—ã—Ç–∏—è –≤–ª–∏—è—é—Ç –Ω–∞ –≤—ã—Ä—É—á–∫—É, –º–∞—Ä–∂—É –∏ –ø—Ä–æ–≥–Ω–æ–∑ –∫–æ–º–ø–∞–Ω–∏–∏.")
    return "\n".join(lines)


def ai_news_analysis(ticker: str, tech_summary: str, news: List[Dict[str, str]]) -> str:
    if not news:
        return "AI-–æ–±–∑–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π: –ø–æ —ç—Ç–æ–º—É —Ç–∏–∫–µ—Ä—É –Ω–µ –Ω–∞—à–ª–æ—Å—å —Å–≤–µ–∂–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤."

    if not OPENAI_API_KEY:
        return fallback_news_summary(news)

    news_block = "\n".join(
        [
            f"{idx + 1}. {n['title']} | {n['publisher']} | {n['date']} | {n['link']}"
            for idx, n in enumerate(news[:5])
        ]
    )

    system_prompt = (
        "–¢—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –î–∞–π –∫—Ä–∞—Ç–∫–∏–π –∏ –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã–π –æ–±–∑–æ—Ä –±–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏—á–Ω—ã—Ö —Å–æ–≤–µ—Ç–æ–≤. "
        "–°—Ç—Ä—É–∫—Ç—É—Ä–∞: 1) –ß—Ç–æ –≤–∞–∂–Ω–æ –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö, 2) –í–æ–∑–º–æ–∂–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –∞–∫—Ü–∏—é, 3) –†–∏—Å–∫–∏, "
        "4) –ß—Ç–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–Ω–≤–µ—Å—Ç–æ—Ä—É. –ü–∏—à–∏ –ø–æ-—Ä—É—Å—Å–∫–∏, –¥–æ 1200 —Å–∏–º–≤–æ–ª–æ–≤."
    )
    user_prompt = (
        f"–¢–∏–∫–µ—Ä: {ticker}\n\n"
        f"–¢–µ—Ö—Å–≤–æ–¥–∫–∞:\n{tech_summary}\n\n"
        f"–ù–æ–≤–æ—Å—Ç–∏:\n{news_block}\n\n"
        "–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫–∏–π AI-–æ–±–∑–æ—Ä."
    )

    payload = {
        "model": OPENAI_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": 0.2,
    }

    req = Request(
        "https://api.openai.com/v1/chat/completions",
        data=json.dumps(payload).encode("utf-8"),
        headers={
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "Content-Type": "application/json",
        },
        method="POST",
    )

    try:
        with urlopen(req, timeout=25) as resp:
            parsed = json.loads(resp.read().decode("utf-8"))
        content = parsed["choices"][0]["message"]["content"].strip()
        return f"AI-–æ–±–∑–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π:\n{content}"
    except (HTTPError, URLError, TimeoutError, KeyError, IndexError, json.JSONDecodeError) as exc:
        logger.warning("OpenAI news analysis failed for %s: %s", ticker, exc)
        return fallback_news_summary(news)


def compute_portfolio_risk(rows: List[Dict[str, float]], total_value: float) -> Dict[str, Optional[float]]:
    tickers = [r["ticker"] for r in rows]
    closes: Dict[str, pd.Series] = {}

    for t in tickers:
        data, _ = load_market_data(t, period="1y", interval="1d", min_rows=30)
        if data is None or "Close" not in data.columns:
            continue
        closes[t] = data["Close"].dropna()

    if len(closes) < 1:
        return {"vol_ann": None, "beta": None, "var_95_usd": None, "var_95_pct": None}

    price_df = pd.DataFrame(closes).dropna(how="any")
    if len(price_df) < 30:
        return {"vol_ann": None, "beta": None, "var_95_usd": None, "var_95_pct": None}

    returns = price_df.pct_change().dropna()
    valid_tickers = [t for t in tickers if t in returns.columns]
    if not valid_tickers:
        return {"vol_ann": None, "beta": None, "var_95_usd": None, "var_95_pct": None}

    weights_map = {
        r["ticker"]: (r["value"] / total_value) if total_value > 0 else 0.0
        for r in rows
        if r["ticker"] in valid_tickers
    }
    weight_sum = sum(weights_map.values())
    if weight_sum <= 0:
        return {"vol_ann": None, "beta": None, "var_95_usd": None, "var_95_pct": None}

    normalized_weights = {k: v / weight_sum for k, v in weights_map.items()}
    w = np.array([normalized_weights[t] for t in valid_tickers])

    port_returns = returns[valid_tickers].dot(w)
    if port_returns.empty:
        return {"vol_ann": None, "beta": None, "var_95_usd": None, "var_95_pct": None}

    vol_ann = float(port_returns.std(ddof=1) * np.sqrt(252) * 100)
    var_95_pct = float(max(0.0, -np.percentile(port_returns, 5) * 100))
    var_95_usd = float(total_value * var_95_pct / 100)

    beta = None
    try:
        spy, _ = load_market_data("SPY", period="1y", interval="1d", min_rows=30)
        if spy is not None and "Close" in spy.columns:
            mkt = spy["Close"].pct_change().dropna().rename("mkt")
            aligned = pd.concat([port_returns.rename("port"), mkt], axis=1).dropna()
            if len(aligned) > 20 and aligned["mkt"].var(ddof=1) > 0:
                cov = aligned[["port", "mkt"]].cov().loc["port", "mkt"]
                beta = float(cov / aligned["mkt"].var(ddof=1))
    except Exception as exc:
        logger.warning("Cannot compute beta: %s", exc)

    return {
        "vol_ann": vol_ann,
        "beta": beta,
        "var_95_usd": var_95_usd,
        "var_95_pct": var_95_pct,
    }


def analyze_portfolio(positions: List[Position]) -> str:
    rows = []
    for p in positions:
        data, _ = load_market_data(p.ticker, period="7d", interval="1d", min_rows=2)
        if data is None or "Close" not in data.columns:
            continue

        close_col = data["Close"]
        if isinstance(close_col, pd.DataFrame):
            close_col = close_col.iloc[:, 0]
        current_price = float(close_col.dropna().iloc[-1])
        market_value = current_price * p.quantity

        pnl_abs = None
        pnl_pct = None
        if p.avg_price and p.avg_price > 0:
            pnl_abs = (current_price - p.avg_price) * p.quantity
            pnl_pct = ((current_price / p.avg_price) - 1) * 100

        rows.append(
            {
                "ticker": p.ticker,
                "qty": p.quantity,
                "avg": p.avg_price,
                "price": current_price,
                "value": market_value,
                "pnl_abs": pnl_abs,
                "pnl_pct": pnl_pct,
            }
        )

    if not rows:
        return (
            "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ –ø–æ—Ä—Ç—Ñ–µ–ª—é. –ü—Ä–æ–≤–µ—Ä—å —Ñ–æ—Ä–º–∞—Ç –∏ —Ç–∏–∫–µ—Ä—ã.\n"
            "–ü—Ä–∏–º–µ—Ä: AAPL 5 170"
        )

    total_value = sum(r["value"] for r in rows)
    risk = compute_portfolio_risk(rows, total_value)

    lines = ["–ê–Ω–∞–ª–∏–∑ –ø–æ—Ä—Ç—Ñ–µ–ª—è", f"–¢–µ–∫—É—â–∞—è –æ—Ü–µ–Ω–∫–∞: {total_value:,.2f}", ""]

    for r in sorted(rows, key=lambda x: x["value"], reverse=True):
        weight = (r["value"] / total_value) * 100 if total_value > 0 else 0
        if r["pnl_abs"] is None:
            pnl_line = "PnL: n/a"
        else:
            pnl_line = f"PnL: {r['pnl_abs']:+.2f} ({r['pnl_pct']:+.2f}%)"

        lines.append(
            f"- {r['ticker']}: qty {r['qty']}, price {r['price']:.2f}, value {r['value']:.2f} ({weight:.1f}%), {pnl_line}"
        )

    lines.append("")
    lines.append("–†–∏—Å–∫-–º–µ—Ç—Ä–∏–∫–∏ (1Y):")
    if risk["vol_ann"] is None:
        lines.append("- –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Ä–∏—Å–∫–∞.")
    else:
        lines.append(f"- –ì–æ–¥–æ–≤–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: {risk['vol_ann']:.2f}%")
        lines.append(
            f"- –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π VaR 95% (1 –¥–µ–Ω—å): {risk['var_95_pct']:.2f}% (~{risk['var_95_usd']:.2f})"
        )
        if risk["beta"] is None:
            lines.append("- –ë–µ—Ç–∞ –∫ SPY: n/a")
        else:
            lines.append(f"- –ë–µ—Ç–∞ –∫ SPY: {risk['beta']:.2f}")

    top_weight = max((r["value"] / total_value) * 100 for r in rows)
    lines.append("")
    lines.append("–ß—Ç–æ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å:")
    if top_weight > 40:
        lines.append("- –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è –≤—ã—Å–æ–∫–∞—è: –æ–¥–Ω–∞ –ø–æ–∑–∏—Ü–∏—è >40%. –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—é.")
    else:
        lines.append("- –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è —É–º–µ—Ä–µ–Ω–Ω–∞—è: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –±–ª–∏–∑–∫–∞ –∫ –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤–æ–π.")

    if risk["vol_ann"] is not None and risk["vol_ann"] > 35:
        lines.append("- –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –≤—ã—Å–æ–∫–∞—è: —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –¥–æ–ª—é —Å–∞–º—ã—Ö —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã—Ö –±—É–º–∞–≥ –∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –∑–∞—â–∏—Ç–Ω—ã–µ –∞–∫—Ç–∏–≤—ã.")

    if risk["beta"] is not None and risk["beta"] > 1.2:
        lines.append("- –ë–µ—Ç–∞ –≤—ã—à–µ —Ä—ã–Ω–∫–∞: –ø–æ—Ä—Ç—Ñ–µ–ª—å —Å–∏–ª—å–Ω–µ–µ —Ä–µ–∞–≥–∏—Ä—É–µ—Ç –Ω–∞ –ø–∞–¥–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞.")

    losers = [r for r in rows if r["pnl_pct"] is not None and r["pnl_pct"] < -10]
    if losers:
        lines.append("- –ï—Å—Ç—å –ø–æ–∑–∏—Ü–∏–∏ —Å –ø—Ä–æ—Å–∞–¥–∫–æ–π >10%: –ø–æ–ª–µ–∑–Ω–æ –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ–∑–∏—Å.")

    gainers = [r for r in rows if r["pnl_pct"] is not None and r["pnl_pct"] > 25]
    if gainers:
        lines.append("- –ï—Å—Ç—å –ª–∏–¥–µ—Ä—ã >25%: –º–æ–∂–Ω–æ —á–∞—Å—Ç–∏—á–Ω–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å –∏ —Ä–µ–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–ª–∏.")

    lines.append("")
    lines.append("–ù–µ —è–≤–ª—è–µ—Ç—Å—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–æ–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–µ–π.")

    return "\n".join(lines)


async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    await update.message.reply_text(
        "–Ø —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –∞–∫—Ü–∏—è–º.\n"
        "–ú–æ–≥—É —Å–¥–µ–ª–∞—Ç—å —Ç–µ—Ö–∞–Ω–∞–ª–∏–∑ –∞–∫—Ü–∏–∏, AI-–æ–±–∑–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —Ä–∞–∑–±–æ—Ä –ø–æ—Ä—Ç—Ñ–µ–ª—è.\n\n"
        "–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ –∫–Ω–æ–ø–∫–æ–π –Ω–∏–∂–µ.",
        reply_markup=main_keyboard(),
    )
    return CHOOSING


async def help_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    await update.message.reply_text(
        "–§–æ—Ä–º–∞—Ç—ã –≤–≤–æ–¥–∞:\n"
        "1) –ê–Ω–∞–ª–∏–∑ –∞–∫—Ü–∏–∏: –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–∏–∫–µ—Ä, –Ω–∞–ø—Ä–∏–º–µ—Ä AAPL –∏–ª–∏ MSFT.\n"
        "2) –ü–æ—Ä—Ç—Ñ–µ–ª—å: –ø–æ –æ–¥–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏ –≤ —Å—Ç—Ä–æ–∫–µ: TICKER QTY AVG_PRICE\n"
        "   –ü—Ä–∏–º–µ—Ä:\n"
        "   AAPL 10 170\n"
        "   MSFT 4 320\n"
        "   TSLA 3\n\n"
        "3) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–∫—Ü–∏–π: 2-5 —Ç–∏–∫–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª –∏–ª–∏ –∑–∞–ø—è—Ç—É—é\n"
        "   –ü—Ä–∏–º–µ—Ä: AAPL MSFT GOOGL\n\n"
        "–ö–Ω–æ–ø–∫–∞ '–ú–æ–π –ø–æ—Ä—Ç—Ñ–µ–ª—å' –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ.\n"
        "–ö–Ω–æ–ø–∫–∞ –û—Ç–º–µ–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤ –º–µ–Ω—é.",
        reply_markup=main_keyboard(),
    )
    return CHOOSING


async def handle_portfolio_from_text(update: Update, text: str, user_id: int) -> int:
    positions = parse_portfolio_text(text)
    if not positions:
        await update.message.reply_text(
            "–ù–µ —Å–º–æ–≥ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—å. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–æ—Ä–º–∞—Ç:\nAAPL 10 170"
        )
        return WAITING_PORTFOLIO

    save_portfolio(user_id, text)
    await update.message.reply_text("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –ø–æ—Ä—Ç—Ñ–µ–ª—å...")
    result = analyze_portfolio(positions)
    await update.message.reply_text(result)
    return WAITING_PORTFOLIO


async def on_choice(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    text = (update.message.text or "").strip()

    if text == MENU_STOCK:
        await update.message.reply_text(
            "–û—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–∏–∫–µ—Ä –∞–∫—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä: AAPL).", reply_markup=main_keyboard()
        )
        return WAITING_STOCK

    if text == MENU_PORTFOLIO:
        await update.message.reply_text(
            "–û—Ç–ø—Ä–∞–≤—å—Ç–µ –ø–æ—Ä—Ç—Ñ–µ–ª—å —Å–ø–∏—Å–∫–æ–º, –∫–∞–∂–¥–∞—è –ø–æ–∑–∏—Ü–∏—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏:\n"
            "TICKER QTY AVG_PRICE\n"
            "–ü—Ä–∏–º–µ—Ä:\n"
            "AAPL 10 170\nMSFT 4 320",
            reply_markup=main_keyboard(),
        )
        return WAITING_PORTFOLIO
    
    if text == MENU_COMPARE:
        await update.message.reply_text(
            "–û—Ç–ø—Ä–∞–≤—å—Ç–µ 2-5 —Ç–∏–∫–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª –∏–ª–∏ –∑–∞–ø—è—Ç—É—é –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.\n"
            "–ü—Ä–∏–º–µ—Ä: AAPL MSFT GOOGL\n"
            "–∏–ª–∏: TSLA, NFLX, NVDA",
            reply_markup=main_keyboard(),
        )
        return WAITING_COMPARISON

    if text == MENU_MY_PORTFOLIO:
        user_id = update.effective_user.id
        saved = get_saved_portfolio(user_id)
        if not saved:
            await update.message.reply_text(
                "–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –ø–æ—Ä—Ç—Ñ–µ–ª—è –ø–æ–∫–∞ –Ω–µ—Ç. –°–Ω–∞—á–∞–ª–∞ –Ω–∞–∂–º–∏—Ç–µ '–ê–Ω–∞–ª–∏–∑ –ø–æ—Ä—Ç—Ñ–µ–ª—è' –∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Å–ø–∏—Å–æ–∫."
            )
            return CHOOSING
        await update.message.reply_text("–ó–∞–≥—Ä—É–∂–∞—é —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø–æ—Ä—Ç—Ñ–µ–ª—å...")
        return await handle_portfolio_from_text(update, saved, user_id)

    if text == MENU_HELP:
        return await help_cmd(update, context)

    if text == MENU_CANCEL:
        await update.message.reply_text("–í–æ–∑–≤—Ä–∞—Ç –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é.", reply_markup=main_keyboard())
        return CHOOSING

    await update.message.reply_text("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ –∫–Ω–æ–ø–∫–æ–π.", reply_markup=main_keyboard())
    return CHOOSING


async def on_stock_input(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    text = (update.message.text or "").strip()
    
    # Check if it's a menu button BEFORE processing as ticker
    if text in {MENU_CANCEL, MENU_HELP, MENU_STOCK, MENU_PORTFOLIO, MENU_MY_PORTFOLIO, MENU_COMPARE}:
        return await on_choice(update, context)
    
    ticker = text.upper().replace("$", "")

    if not re.fullmatch(r"[A-Z0-9.\-]{1,12}", ticker):
        await update.message.reply_text("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–∏–∫–µ—Ä. –ü—Ä–∏–º–µ—Ä: AAPL")
        return WAITING_STOCK

    await update.message.reply_text(f"–°–æ–±–∏—Ä–∞—é –¥–∞–Ω–Ω—ã–µ –ø–æ {ticker}...")

    df, reason = stock_snapshot(ticker)
    if df is None:
        if reason == "rate_limit":
            await update.message.reply_text(
                "–û—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∫–æ—Ç–∏—Ä–æ–≤–æ–∫ –≤—Ä–µ–º–µ–Ω–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–ª –∑–∞–ø—Ä–æ—Å—ã (rate limit).\n"
                "–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (SEC EDGAR, Stooq) —Ç–∞–∫–∂–µ –Ω–µ —É–¥–∞–ª–∞—Å—å.\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ 1-2 –º–∏–Ω—É—Ç—ã –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Ç–∏–∫–µ—Ä."
            )
        else:
            await update.message.reply_text(
                "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ —Ç–∏–∫–µ—Ä—É. –ü—Ä–æ–≤–µ—Ä—å —Å–∏–º–≤–æ–ª –∏ –±–∏—Ä–∂–µ–≤–æ–π —Å—É—Ñ—Ñ–∏–∫—Å.\n"
                "–ü—Ä–∏–º–µ—Ä—ã: AAPL (US), NABL.NS (India), VOD.L (UK)."
            )
        return WAITING_STOCK

    technical = stock_analysis_text(ticker, df)
    chart_path = render_stock_chart(ticker, df)

    news = ticker_news(ticker)
    ai_text = ai_news_analysis(ticker, technical, news)
    final_caption = f"{technical}\n\n–ù–µ —è–≤–ª—è–µ—Ç—Å—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–æ–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–µ–π."

    with open(chart_path, "rb") as f:
        await update.message.reply_photo(photo=f, caption=final_caption[:1000])

    await update.message.reply_text(ai_text[:3500])

    if news:
        lines = ["–°—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏:"]
        for item in news[:5]:
            source = f"{item['publisher']} {item['date']}".strip()
            lines.append(f"- {item['title']} ({source})")
            if item["link"]:
                lines.append(item["link"])
        await update.message.reply_text("\n".join(lines)[:3500])
    else:
        await update.message.reply_text(
            "–°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Ç–∏–∫–µ—Ä—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º, –Ω–∏ –≤ —Ä–µ–∑–µ—Ä–≤–Ω–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–µ."
        )

    try:
        os.remove(chart_path)
    except OSError:
        pass

    return WAITING_STOCK


async def on_portfolio_input(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    text = (update.message.text or "").strip()
    
    # Check if it's a menu button BEFORE processing as portfolio
    if text in {MENU_CANCEL, MENU_HELP, MENU_STOCK, MENU_PORTFOLIO, MENU_MY_PORTFOLIO, MENU_COMPARE}:
        return await on_choice(update, context)

    user_id = update.effective_user.id
    return await handle_portfolio_from_text(update, text, user_id)


async def on_comparison_input(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    text = (update.message.text or "").strip()
    
    # Check if it's a menu button BEFORE processing as tickers
    if text in {MENU_CANCEL, MENU_HELP, MENU_STOCK, MENU_PORTFOLIO, MENU_MY_PORTFOLIO, MENU_COMPARE}:
        return await on_choice(update, context)
    
    # Parse tickers (space or comma separated)
    tickers = re.split(r'[,\s]+', text.upper())
    tickers = [t.strip().replace("$", "") for t in tickers if t.strip()]
    
    # Validate tickers
    valid_tickers = []
    for ticker in tickers:
        if re.fullmatch(r"[A-Z0-9.\-]{1,12}", ticker):
            valid_tickers.append(ticker)
    
    if len(valid_tickers) < 2:
        await update.message.reply_text(
            "–ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Ç–∏–∫–µ—Ä–∞.\n–ü—Ä–∏–º–µ—Ä: AAPL MSFT GOOGL"
        )
        return WAITING_COMPARISON
    
    if len(valid_tickers) > 5:
        await update.message.reply_text(
            "–ú–∞–∫—Å–∏–º—É–º 5 —Ç–∏–∫–µ—Ä–æ–≤ –∑–∞ —Ä–∞–∑.\n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ."
        )
        return WAITING_COMPARISON
    
    await update.message.reply_text(f"–°—Ä–∞–≤–Ω–∏–≤–∞—é {', '.join(valid_tickers)}...")
    
    chart_path, result_text = compare_stocks(valid_tickers, period="6mo")
    
    if chart_path is None:
        await update.message.reply_text(f"–û—à–∏–±–∫–∞: {result_text}")
        return WAITING_COMPARISON
    
    # Send chart
    with open(chart_path, "rb") as f:
        await update.message.reply_photo(photo=f, caption=result_text[:1000])
    
    # Clean up
    try:
        os.remove(chart_path)
    except OSError:
        pass
    
    return WAITING_COMPARISON


async def my_portfolio_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user_id = update.effective_user.id
    saved = get_saved_portfolio(user_id)
    if not saved:
        await update.message.reply_text("–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –ø–æ—Ä—Ç—Ñ–µ–ª—è –Ω–µ—Ç. –°–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –µ–≥–æ —á–µ—Ä–µ–∑ '–ê–Ω–∞–ª–∏–∑ –ø–æ—Ä—Ç—Ñ–µ–ª—è'.")
        return
    await update.message.reply_text("–ó–∞–≥—Ä—É–∂–∞—é —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø–æ—Ä—Ç—Ñ–µ–ª—å...")
    positions = parse_portfolio_text(saved)
    if not positions:
        await update.message.reply_text("–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø–æ—Ä—Ç—Ñ–µ–ª—å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω. –û—Ç–ø—Ä–∞–≤—å—Ç–µ –µ–≥–æ –∑–∞–Ω–æ–≤–æ.")
        return
    result = analyze_portfolio(positions)
    await update.message.reply_text(result)


async def cancel(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    await update.message.reply_text("–î–∏–∞–ª–æ–≥ –∑–∞–≤–µ—Ä—à—ë–Ω.", reply_markup=ReplyKeyboardRemove())
    return ConversationHandler.END


async def cache_stats_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Show cache statistics."""
    market_size = len(market_data_cache.cache)
    news_size = len(news_cache.cache)
    
    stats = (
        f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞:\n\n"
        f"–ö–æ—Ç–∏—Ä–æ–≤–æ–∫ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–æ: {market_size}\n"
        f"–ù–æ–≤–æ—Å—Ç–µ–π –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–æ: {news_size}\n"
        f"TTL –∫–æ—Ç–∏—Ä–æ–≤–æ–∫: {MARKET_DATA_CACHE_TTL}—Å ({MARKET_DATA_CACHE_TTL//60}–º)\n"
        f"TTL –Ω–æ–≤–æ—Å—Ç–µ–π: {NEWS_CACHE_TTL}—Å ({NEWS_CACHE_TTL//60}–º)\n\n"
        f"–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /clearcache –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞"
    )
    await update.message.reply_text(stats)


async def clear_cache_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Clear all cache."""
    market_data_cache.clear()
    news_cache.clear()
    await update.message.reply_text("‚úÖ –ö—ç—à –æ—á–∏—â–µ–Ω!")
    logger.info("Cache cleared by user %s", update.effective_user.id)


async def on_error(update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
    logger.exception("Unhandled error while processing update: %s", context.error)
    if isinstance(update, Update) and update.effective_message:
        try:
            await update.effective_message.reply_text(
                "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥."
            )
        except Exception:
            pass


def build_app(token: str) -> Application:
    app = Application.builder().token(token).build()

    conv = ConversationHandler(
        entry_points=[CommandHandler("start", start)],
        states={
            CHOOSING: [
                CommandHandler("start", start),
                CommandHandler("help", help_cmd),
                MessageHandler(filters.TEXT & ~filters.COMMAND, on_choice),
            ],
            WAITING_STOCK: [
                CommandHandler("start", start),
                CommandHandler("help", help_cmd),
                MessageHandler(filters.TEXT & ~filters.COMMAND, on_stock_input),
            ],
            WAITING_PORTFOLIO: [
                CommandHandler("start", start),
                CommandHandler("help", help_cmd),
                MessageHandler(filters.TEXT & ~filters.COMMAND, on_portfolio_input)
            ],
            WAITING_COMPARISON: [
                CommandHandler("start", start),
                CommandHandler("help", help_cmd),
                MessageHandler(filters.TEXT & ~filters.COMMAND, on_comparison_input)
            ],
        },
        fallbacks=[CommandHandler("cancel", cancel)],
        allow_reentry=True,
    )

    app.add_handler(conv)
    app.add_handler(CommandHandler("help", help_cmd))
    app.add_handler(CommandHandler("myportfolio", my_portfolio_cmd))
    app.add_handler(CommandHandler("cachestats", cache_stats_cmd))
    app.add_handler(CommandHandler("clearcache", clear_cache_cmd))
    app.add_error_handler(on_error)
    return app


def main() -> None:
    load_dotenv()

    global DB_PATH, OPENAI_API_KEY, OPENAI_MODEL
    DB_PATH = os.getenv("PORTFOLIO_DB_PATH", "portfolio.db")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
    OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini").strip() or "gpt-4o-mini"

    token = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
    if not token:
        raise RuntimeError("TELEGRAM_BOT_TOKEN is not set. Put it in .env file.")

    init_db()

    logger.info("Starting bot at %s", datetime.now(timezone.utc).isoformat())
    app = build_app(token)
    
    # Graceful shutdown for Render.com (handle SIGTERM)
    def sig_handler(signum, frame):
        logger.info("Signal %d received, shutting down gracefully...", signum)
        app.stop()
        sys.exit(0)
    
    signal.signal(signal.SIGTERM, sig_handler)
    signal.signal(signal.SIGINT, sig_handler)
    
    try:
        app.run_polling(close_loop=False)
    except KeyboardInterrupt:
        logger.info("Keyboard interrupt received, shutting down...")
    except Exception as e:
        logger.error("Unexpected error: %s", e)
        sys.exit(1)


if __name__ == "__main__":
    main()
